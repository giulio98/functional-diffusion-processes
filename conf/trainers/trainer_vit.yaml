_target_: functional_diffusion_processes.trainers.trainer.Trainer

mode: "train" # "train" for train only, "eval" for evaluation only, "train_eval" for both

model_name: "local" # if "local" search for checkpoint in local directory otherwise from wandb

training_config:
  seed: 42 # random seed for reproducibility
  n_jitted_steps: 1 # number of steps to be jitted
  total_steps: 100000 # total number of training steps
  log_freq: 1 # log loss is computed every log_freq steps
  sampling: True # if True, sampling during training is enabled
  sampling_only: False # if True, sampling is enabled only during evaluation
  eval_freq: 10000 # eval loss is computed every eval_freq steps
  checkpoint_freq: 25000 # checkpoint is saved every checkpoint_freq steps
  resume_training: True # if True, resume training from last checkpoint
  checkpoint_dir: "checkpoints" # directory where model checkpoints are saved
  sample_dir: "samples" # directory where samples are saved
  learning_rate: 1e-3 # learning rate for the training loop
  ema_rate: 0.9999 # exponential moving average rate for the outer loop
  gradient_accumulation_steps: 1 # gradient accumulation steps for the outer loop
  save_dir: ${oc.env:LOGS_ROOT}/run_vit
  scheduler_steps: 30000
  warmup_steps: 6000
  decay_steps: 10000
  peak_value: 3e-3
  end_value: 1e-3
  weight_decay: 0.3
  sampling_type: "full"


optimizer:
  _target_: "optax.MultiSteps"
  opt:
    _target_: "optax.chain"
    _args_:
       #  - _target_: "optax.clip"
       #   max_delta: 1.0
        - _target_: "optax.adabelief"
          learning_rate:
              _target_: "optax.warmup_cosine_decay_schedule"
              init_value: 0.0
              peak_value: ${trainers.training_config.peak_value}
              warmup_steps: ${trainers.training_config.warmup_steps}
              decay_steps: ${trainers.training_config.decay_steps}
              end_value: ${trainers.training_config.end_value}
  every_k_schedule: ${trainers.training_config.gradient_accumulation_steps}

evaluation_config:
  seed: 43 # random seed for reproducibility
  eval_dir: "eval" # directory where evaluation results are saved
  num_samples: 8 # number of samples to be generated for evaluation


trainer_logging:
  use_wandb: True # if True, wandb is used for logging
  wandb_init:
    name: ${trainers.training_config.save_dir}
    project: "your_project"
    entity: "your_entity"
    save_code: False
